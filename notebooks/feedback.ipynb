{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a404e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from mlblocks import MLPipeline\n",
    "from orion.data import load_signal\n",
    "from orion.analysis import _build_events_df\n",
    "from orion.evaluation import contextual_f1_score\n",
    "\n",
    "from sintel.actions import annotator, add_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbe9930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "DATA_PATH = '../sintel/data/{}'\n",
    "\n",
    "UNSUPERVISED_PIPELINE_DIR = '../benchmark/pipelines/{}'\n",
    "\n",
    "ANOMALIES = pd.read_csv(DATA_PATH.format('anomalies.csv'))\n",
    "\n",
    "UNSUPERVISED_PIPELINE = glob(UNSUPERVISED_PIPELINE_DIR.format('*.pkl'))\n",
    "\n",
    "BENCHMARK_DATA = pd.read_csv(\n",
    "    DATA_PATH.format('datasets.csv'), index_col=0, header=None).applymap(ast.literal_eval).to_dict()[1]\n",
    "\n",
    "\n",
    "INTERVAL = {\n",
    "    \"realTweets\": 300\n",
    "}\n",
    "\n",
    "INTERVAL_FMT = {\n",
    "    \"realTweets\": 3600\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a76154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_anomalies(signal):\n",
    "    anomalies = ANOMALIES.set_index('signal').loc[signal].values[0]\n",
    "    anomalies = pd.DataFrame(json.loads(anomalies), columns=['start', 'end'])\n",
    "    return anomalies\n",
    "\n",
    "def _build_events(events):\n",
    "    events = pd.DataFrame(list(events), columns=['start', 'end'])\n",
    "    events['start'] = events['start'].astype(int)\n",
    "    events['end'] = events['end'].astype(int)\n",
    "\n",
    "    return events\n",
    "\n",
    "\n",
    "def _merge_sequences(sequences):\n",
    "    if len(sequences) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    if not isinstance(sequences, list):\n",
    "        sequences = list(sequences[['start', 'end']].itertuples(index=False))\n",
    "\n",
    "    sorted_sequences = sorted(sequences, key=lambda entry: entry[0])\n",
    "    new_sequences = [sorted_sequences[0]]\n",
    "    weights = [sorted_sequences[0][1] - sorted_sequences[0][0]]\n",
    "\n",
    "    for sequence in sorted_sequences[1:]:\n",
    "        prev_sequence = new_sequences[-1]\n",
    "\n",
    "        if sequence[0] <= prev_sequence[1] + 1:\n",
    "            weights.append(sequence[1] - sequence[0])\n",
    "            new_sequences[-1] = (prev_sequence[0], max(prev_sequence[1], sequence[1]))\n",
    "\n",
    "        else:\n",
    "            weights = [sequence[1] - sequence[0]]\n",
    "            new_sequences.append(sequence)\n",
    "\n",
    "    return np.array(new_sequences)\n",
    "\n",
    "def _merge_events(first, second):\n",
    "    first = first.copy()[['start', 'end']]\n",
    "    second = second.copy()[['start', 'end']]\n",
    "    \n",
    "    events = pd.concat([first, second])\n",
    "    return _merge_sequences(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb14aa",
   "metadata": {},
   "source": [
    "### feedback experiment process\n",
    "\n",
    "1. use `lstm_dynamic_threshold` to find anomalies using unsupervised learning.\n",
    "2. use `simulation` to select a number of fp, fn to correct by the simulator in the train dataset:\n",
    "    - this selection can be weighted by severity score\n",
    "    - this selection can be random\n",
    "3. train the `lstm_supervised` pipeline on the annotated timeseries\n",
    "4. evaluate the performance on the test dataset.\n",
    "5. repeat step 2 - 4.\n",
    "\n",
    "**excluding**\n",
    "NASA dataset due to the fact that nasa dataset is a pre-split into (train: no anomalies, test: with anomalies), therefore, the simulation will need to be executed on the test dataset which is illogical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c79cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomalies_in_test(anomalies, start):\n",
    "    df = anomalies.copy()\n",
    "    remove = []\n",
    "    for i, anom in df.iterrows():\n",
    "        if anom.start < start and anom.end > start:\n",
    "            df.at[i, 'start'] = start\n",
    "        elif anom.start < start:\n",
    "            remove.append(i)\n",
    "\n",
    "    return df.drop(remove)\n",
    "\n",
    "def get_anomalies_in_train(anomalies, start):\n",
    "    df = anomalies.copy()\n",
    "    remove = []\n",
    "    for i, anom in df.iterrows():\n",
    "        if anom.start < start and anom.end > start:\n",
    "            df.at[i, 'end'] = start\n",
    "        elif anom.start > start:\n",
    "            remove.append(i)\n",
    "\n",
    "    return df.drop(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfbed143",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'realTweets'\n",
    "\n",
    "for signal in BENCHMARK_DATA[dataset]:\n",
    "    \n",
    "# data\n",
    "signal = 'Twitter_volume_AAPL'\n",
    "\n",
    "data_path = DATA_PATH.format(signal + '.csv')\n",
    "timeseries = load_signal(data_path)\n",
    "train, test = load_signal(data_path, test_size=0.3)\n",
    "ground_truth = load_anomalies(signal)\n",
    "\n",
    "# train\n",
    "train['label'] = [0] * len(train)\n",
    "train = train.set_index('timestamp').sort_index()  \n",
    "train = train.reset_index()\n",
    "\n",
    "# test\n",
    "test['label'] = [0] * len(test)\n",
    "test = test.set_index('timestamp').sort_index()  \n",
    "test = test.reset_index()\n",
    "\n",
    "# unsupervised\n",
    "\n",
    "pipeline_path = [x for x in UNSUPERVISED_PIPELINE if signal in x][0]\n",
    "\n",
    "with open(pipeline_path, 'rb') as pfile:\n",
    "    pipeline = pickle.load(pfile)\n",
    "    \n",
    "anomalies = pipeline.predict(timeseries)\n",
    "anomalies = _build_events_df(anomalies)\n",
    "\n",
    "start = test['timestamp'].min()\n",
    "\n",
    "# train \n",
    "train_anomalies = get_anomalies_in_train(anomalies, start)\n",
    "train_ground_truth = get_anomalies_in_train(ground_truth, start)\n",
    "\n",
    "# test \n",
    "test_anomalies = get_anomalies_in_test(anomalies, start)\n",
    "test_ground_truth = get_anomalies_in_test(ground_truth, start)\n",
    "\n",
    "class_weights = {0: 1, 1: 1e3}\n",
    "interval = INTERVAL[dataset]\n",
    "interval_fmt = INTERVAL_FMT[dataset]\n",
    "\n",
    "hyperparameters = {\n",
    "    \"mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n",
    "        \"interval\": interval,\n",
    "    },\n",
    "    \"keras.Sequential.LSTMTimeSeriesClassifier#1\": {\n",
    "        \"epochs\": 10,\n",
    "        'validation_split': 0.0,\n",
    "        'verbose': False\n",
    "    },\n",
    "    \"sintel.primitives.timeseries_anomalies.format_anomalies#1\": {\n",
    "        \"interval\": interval_fmt\n",
    "    }\n",
    "}\n",
    "\n",
    "supervised_pipeline = MLPipeline(\"lstm_supervised\")\n",
    "supervised_pipeline.set_hyperparameters(hyperparameters)\n",
    "\n",
    "# simulate annotation\n",
    "\n",
    "k = 1\n",
    "events = _merge_sequences(annotator(train_anomalies, train_ground_truth, k))\n",
    "stop = contextual_f1_score(events, train_ground_truth, weighted=False) == 1.0\n",
    "\n",
    "while not stop:\n",
    "\n",
    "    train = train.set_index('timestamp').sort_index()\n",
    "    for a in events:\n",
    "        train.at[a.start: a.end, 'label'] = 1\n",
    "\n",
    "    train = train.reset_index()\n",
    "    train.head()\n",
    "\n",
    "    supervised_pipeline.fit(train, class_weights=class_weights)\n",
    "\n",
    "    # train score\n",
    "    train_anomalies = supervised_pipeline.predict(train)\n",
    "    train_anomalies = _build_events(train_anomalies)\n",
    "\n",
    "    train_f1_score = contextual_f1_score(train_anomalies, train_ground_truth, weighted=False)\n",
    "\n",
    "    # test score\n",
    "    test_anomalies = supervised_pipeline.predict(test)\n",
    "    test_anomalies = _build_events(test_anomalies)\n",
    "\n",
    "    test_f1_score = contextual_f1_score(test_anomalies, test_ground_truth, weighted=False)\n",
    "    \n",
    "    # display\n",
    "    print(\"train score: \", train_f1_score)\n",
    "    print(\"test score: \", test_f1_score)\n",
    "    \n",
    "    events = _merge_sequences(annotator(train_anomalies, train_ground_truth, k))\n",
    "    stop = contextual_f1_score(events, train_ground_truth, weighted=False) == 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
